//#include <graphcode.h>
//#include <cudahelper.cuh>
#include <cuda_runtime.h>

#include <c++/9/iostream>
#include "../src/cuda_algorithms.cuh"
#include "../src/cudahelper.cuh"


////
//// Created by breucking on 31.01.22.
////
//
//
//
//
//
////defines the shared memory size
//#include <cudahelper.cuh>
//#include <cassert>
//#include <c++/9/iostream>
//#include <graphcode.h>
//
//#define SHARED_LIMIT 1024
//
//#define GIGA 1073741824
///*
// * division of the vector to be sorted in buckets
// * the attributes of the object Block are the parameters of each bucket
// */
//template <typename Type>
//struct Block
//{
//
//    unsigned int begin;
//    unsigned int end;
//
//    unsigned int nextbegin;
//    unsigned int nextend;
//
//    Type		 pivot;
//
//    //max of the bucket items
//    Type		 maxPiv;
//    //min of the bucket items
//    Type		 minPiv;
//    //done indicates that a bucket has been analyzed
//    short		 done;
//    short		 select;
//
//
//};
//
//
//
//template <typename Type>
//struct Partition
//{
//
//    unsigned int ibucket;
//    unsigned int from;
//    unsigned int end;
//    Type pivot;
//};
//
//typedef unsigned int uint;
//
//size_t scanInclusiveShort(
//        uint *d_Dst,
//        uint *d_Src,
//        uint batchSize,
//        uint arrayLength
//);
//
//size_t scanInclusiveLarge(
//        uint *d_Dst,
//        uint *d_Src,
//        uint batchSize,
//        uint arrayLength
//);
//
//
//void CUDA_Quicksort(Metrics *inData, Metrics *outData, unsigned int dataSize, unsigned int threads /*, int Device, double* timer*/);
//
//void CUDA_Quicksort_64(double* inData,double* outData, unsigned int size, unsigned int threads, int Device, double* timer);
//
//typedef unsigned int Type;
//
//void test_bitonicSort(unsigned int* h_InputKey,unsigned int N, double* timer);
//void test_MergeSort  (unsigned int*h_SrcKey   ,unsigned int N, double* timer);
//void test_thrustSort (Type* h_data    ,unsigned int N, double* timer);
//
//
void testCudaSort();
//
//int main() {
//    testCudaSort();
//}

////////////////////////////////////////////////////////////////////////////////
// Initialize data on the host.
////////////////////////////////////////////////////////////////////////////////
void initialize_data(unsigned int *dst, unsigned int nitems) {
    // Fixed seed for illustration
    srand(2047);

    // Fill dst with random values
    for (unsigned i = 0; i < nitems; i++)
        dst[i] = rand() % nitems;
}
////////////////////////////////////////////////////////////////////////////////
// Main entry point.
////////////////////////////////////////////////////////////////////////////////
int main(int argc, char **argv) {
    int num_items = 128;
    bool verbose = false;


    verbose = true;
    num_items = 100000;



    // Get device properties
    int device_count = 0, device = 0;
    cudaDeviceProp properties;
    HANDLE_ERROR(cudaGetDeviceProperties(&properties, device));

    if (properties.major > 3 || (properties.major == 3 && properties.minor >= 5)) {
        std::cout << "Running on GPU " << device << " (" << properties.name << ")" << std::endl;
    } else {
        std::cout << "ERROR: cdpsimpleQuicksort requires GPU devices with compute SM 3.5 or higher." << std::endl;
        std::cout << "Current GPU device has compute SM" << properties.major << "." << properties.minor
                  << ". Exiting..." << std::endl;
        exit(EXIT_FAILURE);
    }


    if (device == -1) {
        std::cerr << "cdpSimpleQuicksort requires GPU devices with compute SM 3.5 or higher.  Exiting..." << std::endl;
        exit(EXIT_SUCCESS);
    }

    cudaSetDevice(device);

    // Create input data
    unsigned int *h_data = 0;
    unsigned int *d_data = 0;

    // Allocate CPU memory and initialize data.
    std::cout << "Initializing data:" << std::endl;
    h_data = (unsigned int *) malloc(num_items * sizeof(unsigned int));
    initialize_data(h_data, num_items);

//    if (verbose) {
//        for (int i = 0; i < num_items; i++)
//            std::cout << "Data [" << i << "]: " << h_data[i] << std::endl;
//    }

    // Allocate GPU memory.
    HANDLE_ERROR(cudaMalloc((void **) &d_data, num_items * sizeof(unsigned int)));
    HANDLE_ERROR(cudaMemcpy(d_data, h_data, num_items * sizeof(unsigned int), cudaMemcpyHostToDevice));

    // Execute
    std::cout << "Running quicksort on " << num_items << " elements" << std::endl;
    run_qsort(d_data, num_items);

    // Check result
    std::cout << "Validating results: ";
    check_results(num_items, d_data);

    free(h_data);
    HANDLE_ERROR(cudaFree(d_data));

    // cudaDeviceReset causes the driver to clean up all state. While
    // not mandatory in normal operation, it is good practice.  It is also
    // needed to ensure correct operation when the application is being
    // profiled. Calling cudaDeviceReset causes all profile data to be
    // flushed before the application exits
    cudaDeviceReset();
    exit(EXIT_SUCCESS);
}

//
void testCudaSort() {
//
//    const unsigned int N = 100000;
//    Metrics *inData = new Metrics[N];
//
//    for (int i = 0; i < N; i++) {
//        inData[i].idx = N - i;
//    }
//
//    for (int i = 0; i < N; i++) {
//        // std::cout << "(" << i <<") " << inData[i] << " " ;
//    }
//    Metrics *outData = new Metrics[N];
//    unsigned int num_items = N;
////    CUDA_Quicksort(inData,outData, N,128);
//
//
//    for (int i = 0; i < N; i++) {
//        std::cout << "(" << i << ") " << outData[i].idx << " ";
//    }


}
//
//extern __shared__ uint sMemory[];
//
//
//
////All three kernels run 512 threads per workgroup
////Must be a power of two
//#define THREADBLOCK_SIZE 256
//
//////////////////////////////////////////////////////////////////////////////////
//// Basic ccan codelets
//////////////////////////////////////////////////////////////////////////////////
//#if(0)
////Naive inclusive scan: O(N * log2(N)) operations
//    //Allocate 2 * 'size' local memory, initialize the first half
//    //with 'size' zeros avoiding if(pos >= offset) condition evaluation
//    //and saving instructions
//    inline __device__ uint scan1Inclusive(uint idata,  uint *s_Data, uint size){
//        uint pos = 2 * threadIdx.x - (threadIdx.x & (size - 1));
//        s_Data[pos] = 0;
//        pos += size;
//        s_Data[pos] = idata;
//
//        for(uint offset = 1; offset < size; offset <<= 1){
//            __syncthreads();
//            uint t = s_Data[pos] + s_Data[pos - offset];
//            __syncthreads();
//            s_Data[pos] = t;
//        }
//
//        return s_Data[pos];
//    }
//
//    inline __device__ uint scan1Exclusive(uint idata,  uint *s_Data, uint size){
//        return scan1Inclusive(idata, s_Data, size) - idata;
//    }
//
//#else
//#define LOG2_WARP_SIZE 5U
//#define WARP_SIZE (1U << LOG2_WARP_SIZE)
//
////Almost the same as naive scan1Inclusive, but doesn't need __syncthreads()
////assuming size <= WARP_SIZE
//inline __device__ uint warpScanInclusive(uint idata,  uint *s_Data, uint size){
//    uint pos = 2 * threadIdx.x - (threadIdx.x & (size - 1));
//    s_Data[pos] = 0;
//    pos += size;
//    s_Data[pos] = idata;
//
//    for(uint offset = 1; offset < size; offset <<= 1)
//        s_Data[pos] += s_Data[pos - offset];
//
//    return s_Data[pos];
//}
//
//inline __device__ uint warpScanExclusive(uint idata,  uint *s_Data, uint size){
//    return warpScanInclusive(idata, s_Data, size) - idata;
//}
//
//inline __device__ uint scan1Inclusive(uint idata,  uint *s_Data, uint size){
//    if(size > WARP_SIZE){
//        //Bottom-level inclusive warp scan
//        uint warpResult = warpScanInclusive(idata, s_Data, WARP_SIZE);
//
//        //Save top elements of each warp for exclusive warp scan
//        //sync to wait for warp scans to complete (because s_Data is being overwritten)
//        __syncthreads();
//        if( (threadIdx.x & (WARP_SIZE - 1)) == (WARP_SIZE - 1) )
//            s_Data[threadIdx.x >> LOG2_WARP_SIZE] = warpResult;
//
//        //wait for warp scans to complete
//        __syncthreads();
//        if( threadIdx.x < (THREADBLOCK_SIZE / WARP_SIZE) ){
//            //grab top warp elements
//            uint val = s_Data[threadIdx.x];
//            //calculate exclsive scan and write back to shared memory
//            s_Data[threadIdx.x] = warpScanExclusive(val, s_Data, size >> LOG2_WARP_SIZE);
//        }
//
//        //return updated warp scans with exclusive scan results
//        __syncthreads();
//        return warpResult + s_Data[threadIdx.x >> LOG2_WARP_SIZE];
//    }else{
//        return warpScanInclusive(idata, s_Data, size);
//    }
//}
//
//inline __device__ uint scan1Exclusive(uint idata,  uint *s_Data, uint size){
//    return scan1Inclusive(idata, s_Data, size) - idata;
//}
//
//#endif
//__device__ inline  double atomicMax(double* address, double val)
//{
//    unsigned long long int* address_as_ull = (unsigned long long int*)address;
//    unsigned long long int assumed;
//    unsigned long long int old = *address_as_ull;
//
//    assumed = old;
//    old = atomicCAS(address_as_ull,
//                    assumed,
//                    __double_as_longlong(max(val ,__longlong_as_double(assumed))));
//
//    while (assumed != old)
//    {
//        assumed = old;
//        old = atomicCAS(address_as_ull,
//                        assumed,
//                        __double_as_longlong(max(val ,__longlong_as_double(assumed))));
//    }
//    return __longlong_as_double(old);
//}
//
//
//__device__ inline double atomicMin(double* address, double val)
//{
//    unsigned long long int* address_as_ull = (unsigned long long int*)address;
//    unsigned long long int old = *address_as_ull, assumed;
//
//    assumed = old;
//    old = atomicCAS(address_as_ull,
//                    assumed,
//                    __double_as_longlong(min(val ,__longlong_as_double(assumed))));
//    while (assumed != old)
//    {
//        assumed = old;
//        old = atomicCAS(address_as_ull,
//                        assumed,
//                        __double_as_longlong(min(val ,__longlong_as_double(assumed))));
//    }
//    return __longlong_as_double(old);
//}
//
//
//
//
//
//template <typename Type>
//__device__ inline void Comparator(
//
//        Type& valA,
//        Type& valB,
//        uint dir
//){
//    Type t;
//
//    if( (valA > valB) == dir ){
//        t = valA; valA = valB; valB = t;
//    }
//}
////
////template <typename Type>
////__device__ inline void Comparator(
////// compare(Metrics *positionA, Metrics *positionB) {
////        Metrics positionA,
////        Metrics positionB,
////        uint dir
////){
////
////    //if ( array[ index ].similarity < array[ smallest ].similarity )
////    float a = positionA.similarity * 100000.0f + positionA.recommendation * 100.0f +
////              positionA.inferencing;
////
////
////    float b = positionB.similarity * 100000.0f + positionB.recommendation * 100.0f +
////              positionB.inferencing;
////
////
////        Metrics t;
////
////    if( (a-b > 0) == dir ){
////        t = positionA; positionA = positionB; positionB = t;
////    }
////
////    Type t;
////
////    if( (valA > valB) == dir ){
////        t = valA; valA = valB; valB = t;
////    }
////}
//
//
//
//
//static __device__ __forceinline__ unsigned int __qsflo(unsigned int word)
//{
//    unsigned int ret;
//    asm ("bfind.u32 %0, %1;" : "=r"(ret) : "r"(word));
//    return ret;
//}
//
//template <typename Type>
//__global__ void globalBitonicSort(Type* indata,Type*outdata, Block<Type>* bucket, bool inputSelect)
//{
//    __shared__ uint shared[1024];
//
//
//    Type* data;
//
//    Block<Type> cord = bucket[blockIdx.x];
//
//
//    uint size=cord.end-cord.begin;
//    bool select = !(cord.select);
//
//    if(cord.end-cord.begin>1024 || cord.end-cord.begin==0)
//        return;
//
//    unsigned int bitonicSize = 1 << (__qsflo(size-1U)+1);
//
//
//    if(select)
//        data = indata;
//    else
//        data = outdata;
//
//    //__syncthreads();
//
//    for(int i=threadIdx.x;i<size;i+=blockDim.x)
//        shared[i] = data[i+cord.begin];
//
//
//    for(int i=threadIdx.x+size;i<bitonicSize;i+=blockDim.x)
//        shared[i] = 0xffffffff;
//
//    __syncthreads();
//
//
//    for(uint size = 2; size < bitonicSize; size <<= 1){
//        //Bitonic merge
//        uint ddd = 1 ^ ( (threadIdx.x & (size / 2)) != 0 );
//        for(uint stride = size / 2; stride > 0; stride >>= 1){
//            __syncthreads();
//            uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
//            //if(pos <bitonicSize){
//            Comparator(
//                    shared[pos +      0],
//                    shared[pos + stride],
//                    ddd
//            );
//            // }
//        }
//    }
//
//
//    //ddd == dir for the last bitonic merge step
//
//    for(uint stride = bitonicSize / 2; stride > 0; stride >>= 1){
//        __syncthreads();
//        uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
//        // if(pos <bitonicSize){
//        Comparator(
//                shared[pos +      0],
//                shared[pos + stride],
//                1
//        );
//        // }
//    }
//
//    __syncthreads();
//
//    // Write back the sorted data to its correct position
//    for(int i=threadIdx.x;i<size;i+=blockDim.x)
//        indata[i+cord.begin] = shared[i];
//
//}
//
//
//
//template <typename Type>
//inline __device__ void warpScanInclusive2(Type& idata,Type& idata2,  Type *s_Data, Type *s_Data2, uint size){
//
//    // uint* s_Data2;
//    //s_Data2 = s_Data + blockDim.x*2;
//
//    uint pos = 2 * threadIdx.x - (threadIdx.x & (size - 1));
//    s_Data[pos] = 0;
//    s_Data2[pos] = 0;
//    pos += size;
//    s_Data[pos] = idata;
//    s_Data2[pos] = idata2;
//
//    for(uint offset = 1; offset < size; offset <<= 1)
//    {
//        s_Data[pos] += s_Data[pos - offset];
//        s_Data2[pos] += s_Data2[pos - offset];
//    }
//
//    idata=s_Data[pos];
//    idata2=s_Data2[pos];
//}
//
//template <typename Type>
//inline __device__ void warpScanExclusive2(Type& idata,Type& idata2,  Type *s_Data, Type *s_Data2, uint size){
//
//    // uint* s_Data2;
//    //s_Data2 = s_Data + blockDim.x*2;
//
//    uint pos = 2 * threadIdx.x - (threadIdx.x & (size - 1));
//    s_Data[pos] = 0;
//    s_Data2[pos] = 0;
//    pos += size;
//    s_Data[pos] = idata;
//    s_Data2[pos] = idata2;
//
//    for(uint offset = 1; offset < size; offset <<= 1)
//    {
//        s_Data[pos] += s_Data[pos - offset];
//        s_Data2[pos] += s_Data2[pos - offset];
//    }
//
//    idata=s_Data[pos]-idata;
//    idata2=s_Data2[pos]-idata2;
//}
//
//#define LOG2_WARP_SIZE 5U
//#define WARP_SIZE (1U << LOG2_WARP_SIZE)
//
//template <typename Type>
//inline __device__ void scan1Inclusive2(Type& idata,Type& idata2,  Type *s_Data, uint size){
//
//     Type* s_Data2;
//    s_Data2 = s_Data + blockDim.x*2;
//
//    if(size > WARP_SIZE){
//
//        //Bottom-level inclusive warp scan
//        warpScanInclusive2(idata,idata2, s_Data,s_Data2, WARP_SIZE);
//
//        //Save top Types of each warp for exclusive warp scan
//        //sync to wait for warp scans to complete (because s_Data is being overwritten)
//        __syncthreads();
//        if( (threadIdx.x & (WARP_SIZE - 1)) == (WARP_SIZE - 1) )
//        {
//            s_Data[threadIdx.x >> LOG2_WARP_SIZE] = idata;
//            s_Data2[threadIdx.x >> LOG2_WARP_SIZE] = idata2;
//        }
//
//        //wait for warp scans to complete
//        __syncthreads();
//        if( threadIdx.x < (blockDim.x / WARP_SIZE) ){
//            //grab top warp Types
//            Type val = s_Data[threadIdx.x];
//            Type val2 = s_Data2[threadIdx.x];
//            //calculate exclsive scan and write back to shared memory
//            warpScanExclusive2(val,val2, s_Data,s_Data2, size >> LOG2_WARP_SIZE);
//            s_Data[threadIdx.x] = val;
//            s_Data2[threadIdx.x] = val2;
//        }
//
//        //return updated warp scans with exclusive scan results
//        __syncthreads();
//        idata  += s_Data[threadIdx.x >> LOG2_WARP_SIZE];
//        idata2 += s_Data2[threadIdx.x >> LOG2_WARP_SIZE];
//    }
//    else
//        warpScanInclusive2(idata,idata2, s_Data,s_Data2, size);
//
//}
//
//template <typename Type>
//inline __device__ void warpCompareInclusive(Type& idata,Type& idata2,  Type *s_Data, uint size){
//
//     Type* s_Data2;
//    s_Data2 = s_Data + blockDim.x*2;
//    uint pos = 2 * threadIdx.x - (threadIdx.x & (size - 1));
//    s_Data[pos] = 0;
//    s_Data2[pos] = 0;
//    pos += size;
//    s_Data[pos] = idata;
//    s_Data2[pos] = idata2;
//
//    for(uint offset = 1; offset < size; offset <<= 1)
//    {
//        s_Data[pos] =max(s_Data[pos], s_Data[pos - offset]);
//        s_Data2[pos] =min(s_Data2[pos], s_Data2[pos - offset]);
//    }
//
//    idata = s_Data[pos];
//    idata2= s_Data2[pos];
//}
//
//template <typename Type>
//inline __device__ void compareInclusive(Type& idata,Type& idata2,  Type *s_Data, uint size){
//
//     Type* s_Data2;
//    s_Data2 = s_Data + blockDim.x*2;
//    //Bottom-level inclusive warp scan
//    warpCompareInclusive(idata,idata2, s_Data, WARP_SIZE);
//
//    //Save top Types of each warp for exclusive warp scan
//    //sync to wait for warp scans to complete (because s_Data is being overwritten)
//    __syncthreads();
//    if( (threadIdx.x & (WARP_SIZE - 1)) == (WARP_SIZE - 1) )
//    {
//        s_Data[threadIdx.x >> LOG2_WARP_SIZE] = idata;
//        s_Data2[threadIdx.x >> LOG2_WARP_SIZE] = idata2;
//    }
//
//    //wait for warp scans to complete
//    __syncthreads();
//    if( threadIdx.x < (blockDim.x / WARP_SIZE) ){
//        //grab top warp Types
//        Type val = s_Data[threadIdx.x];
//        Type val2 = s_Data2[threadIdx.x];
//        //calculate exclsive scan and write back to shared memory
//        warpCompareInclusive(val,val2, s_Data, size >> LOG2_WARP_SIZE);
//        s_Data[threadIdx.x] =val;
//        s_Data2[threadIdx.x] =val2;
//    }
//
//    //return updated warp scans with exclusive scan results
//    __syncthreads();
//    idata=max(idata,s_Data[threadIdx.x >> LOG2_WARP_SIZE]) ;
//    idata2=min(idata2,s_Data2[threadIdx.x >> LOG2_WARP_SIZE]) ;
//
//}
//
//
//
//
//inline __device__ uint4 scan4Inclusive(uint4 idata4,  uint *s_Data, uint size){
//    //Level-0 exclusive scan
//    idata4.y += idata4.x;
//    idata4.z += idata4.y;
//    idata4.w += idata4.z;
//
//
//    //Level-1 exclusive scan
//    uint oval = scan1Exclusive(idata4.w, s_Data, size / 4);
//
//    idata4.x += oval;
//    idata4.y += oval;
//    idata4.z += oval;
//    idata4.w += oval;
//
//
//    return idata4;
//}
//
////Exclusive vector scan: the array to be scanned is stored
////in local thread memory scope as uint4
//inline __device__ uint4 scan4Exclusive(uint4 idata4,  uint *s_Data, uint size){
//    uint4 odata4 = scan4Inclusive(idata4, s_Data, size);
//    odata4.x -= idata4.x;
//    odata4.y -= idata4.y;
//    odata4.z -= idata4.z;
//    odata4.w -= idata4.w;
//    return odata4;
//}
//
//////////////////////////////////////////////////////////////////////////////////
//// Scan kernels
//////////////////////////////////////////////////////////////////////////////////
//__global__ void scanExclusiveShared(
//        uint4 *d_Dst,
//        uint4 *d_Src,
//        uint size
//){
//    __shared__ uint s_Data[2 * THREADBLOCK_SIZE];
//
//    uint pos = blockIdx.x * blockDim.x + threadIdx.x;
//
//    //Load data
//    uint4 idata4 = d_Src[pos];
//
//    //Calculate exclusive scan
//    uint4 odata4 = scan4Exclusive(idata4, s_Data, size);
//
//    //Write back
//    d_Dst[pos] = odata4;
//}
//
//
//
////Exclusive scan of top elements of bottom-level scans (4 * THREADBLOCK_SIZE)
//__global__ void scanExclusiveShared2(
//        uint *d_Buf,
//        uint *d_Dst,
//        uint *d_Src,
//        uint N,
//        uint arrayLength
//){
//    __shared__ uint s_Data[2 * THREADBLOCK_SIZE];
//
//    //Skip loads and stores for inactive threads of last threadblock (pos >= N)
//    uint pos = blockIdx.x * blockDim.x + threadIdx.x;
//
//    //Load top elements
//    //Convert results of bottom-level scan back to inclusive
//    uint idata = 0;
//    if(pos < N)
//        idata =
//                d_Dst[(4 * THREADBLOCK_SIZE) - 1 + (4 * THREADBLOCK_SIZE) * pos] +
//                d_Src[(4 * THREADBLOCK_SIZE) - 1 + (4 * THREADBLOCK_SIZE) * pos];
//
//    //Compute
//    uint odata = scan1Exclusive(idata, s_Data, arrayLength);
//
//    //Avoid out-of-bound access
//    if(pos < N)
//        d_Buf[pos] = odata;
//}
//
//
////Final step of large-array scan: combine basic inclusive scan with exclusive scan of top elements of input arrays
//__global__ void uniformUpdate(
//        uint4 *d_Data,
//        uint *d_Buffer
//){
//    __shared__ uint buf;
//    uint pos = blockIdx.x * blockDim.x + threadIdx.x;
//
//    if(threadIdx.x == 0)
//        buf = d_Buffer[blockIdx.x];
//    __syncthreads();
//
//    uint4 data4 = d_Data[pos];
//    data4.x += buf;
//    data4.y += buf;
//    data4.z += buf;
//    data4.w += buf;
//    d_Data[pos] = data4;
//}
//
//////////////////////////////////////////////////////////////////////////////////
//// Interface function
//////////////////////////////////////////////////////////////////////////////////
////Derived as 32768 (max power-of-two gridDim.x) * 4 * THREADBLOCK_SIZE
////Due to scanExclusiveShared<<<>>>() 1D block addressing
//const uint MAX_BATCH_ELEMENTS = 64 * 1048576;
//const uint MIN_SHORT_ARRAY_SIZE = 4;
//const uint MAX_SHORT_ARRAY_SIZE = 4 * THREADBLOCK_SIZE;
//const uint MIN_LARGE_ARRAY_SIZE = 8 * THREADBLOCK_SIZE;
//const uint MAX_LARGE_ARRAY_SIZE = 4 * THREADBLOCK_SIZE * THREADBLOCK_SIZE;
//
////Internal exclusive scan buffer
//static uint *d_Buf;
//
//void initScan(void){
//    HANDLE_ERROR( cudaMalloc((void **)&d_Buf, (MAX_BATCH_ELEMENTS / (4 * THREADBLOCK_SIZE)) * sizeof(uint)) );
//}
//
//void closeScan(void){
//    HANDLE_ERROR( cudaFree(d_Buf) );
//}
//
//static uint factorRadix2(uint& log2L, uint L){
//    if(!L){
//        log2L = 0;
//        return 0;
//    }else{
//        for(log2L = 0; (L & 1) == 0; L >>= 1, log2L++);
//        return L;
//    }
//}
//
//static uint iDivUp(uint dividend, uint divisor){
//    return ( (dividend % divisor) == 0 ) ? (dividend / divisor) : (dividend / divisor + 1);
//}
//
//
//size_t scanExclusiveShort(
//        uint *d_Dst,
//        uint *d_Src,
//        uint batchSize,
//        uint arrayLength
//){
//    //Check power-of-two factorization
//    uint log2L;
//    uint factorizationRemainder = factorRadix2(log2L, arrayLength);
//    assert( factorizationRemainder == 1 );
//
//    //Check supported size range
//    assert( (arrayLength >= MIN_SHORT_ARRAY_SIZE) && (arrayLength <= MAX_SHORT_ARRAY_SIZE) );
//
//    //Check total batch size limit
//    assert( (batchSize * arrayLength) <= MAX_BATCH_ELEMENTS );
//
//    //Check all threadblocks to be fully packed with data
//    assert( (batchSize * arrayLength) % (4 * THREADBLOCK_SIZE) == 0 );
//
//    scanExclusiveShared<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE>>>(
//            (uint4 *)d_Dst,
//            (uint4 *)d_Src,
//            arrayLength
//    );
////    getLastCudaError("scanExclusiveShared() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    return THREADBLOCK_SIZE;
//}
//
//size_t scanExclusiveLarge(
//        uint *d_Dst,
//        uint *d_Src,
//        uint batchSize,
//        uint arrayLength
//){
//    //Check power-of-two factorization
//    uint log2L;
//    uint factorizationRemainder = factorRadix2(log2L, arrayLength);
//    assert( factorizationRemainder == 1 );
//
//    //Check supported size range
//    assert( (arrayLength >= MIN_LARGE_ARRAY_SIZE) && (arrayLength <= MAX_LARGE_ARRAY_SIZE) );
//
//    //Check total batch size limit
//    assert( (batchSize * arrayLength) <= MAX_BATCH_ELEMENTS );
//
//    scanExclusiveShared<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE>>>(
//            (uint4 *)d_Dst,
//            (uint4 *)d_Src,
//            4 * THREADBLOCK_SIZE
//    );
////    getLastCudaError("scanExclusiveShared() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    //Not all threadblocks need to be packed with input data:
//    //inactive threads of highest threadblock just don't do global reads and writes
//    const uint blockCount2 = iDivUp( (batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE );
//    scanExclusiveShared2<<< blockCount2, THREADBLOCK_SIZE>>>(
//            (uint *)d_Buf,
//            (uint *)d_Dst,
//            (uint *)d_Src,
//            (batchSize * arrayLength) / (4 * THREADBLOCK_SIZE),
//            arrayLength / (4 * THREADBLOCK_SIZE)
//    );
////    getLastCudaError("scanExclusiveShared2() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    uniformUpdate<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE>>>(
//            (uint4 *)d_Dst,
//            (uint  *)d_Buf
//    );
////    getLastCudaError("uniformUpdate() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    return THREADBLOCK_SIZE;
//}
//
/////////////////////////////////////////////////////////////////////////////////////////7
/////// work derived by NVIDIA Corporation
/////////////////////////////////////////////////////////////////////////////////////////
//
///*
// *
// * scan.cu
// *
// * Copyright © 2012-2015 Emanuele Manca
// *
// **********************************************************************************************
// **********************************************************************************************
// *
// 	This file is part of CUDA-Quicksort.
//
//    CUDA-Quicksort is free software: you can redistribute it and/or modify
//    it under the terms of the GNU General Public License as published by
//    the Free Software Foundation, either version 3 of the License, or
//    (at your option) any later version.
//
//    CUDA-Quicksort is distributed in the hope that it will be useful,
//    but WITHOUT ANY WARRANTY; without even the implied warranty of
//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//    GNU General Public License for more details.
//
//    You should have received a copy of the GNU General Public License
//    along with CUDA-Quicksort.
//
//    If not, see http://www.gnu.org/licenses/gpl-3.0.txt and http://www.gnu.org/copyleft/gpl.html
//
//
//  **********************************************************************************************
//  **********************************************************************************************
// *
// * Contact: Ing. Emanuele Manca
// *
// * Department of Electrical and Electronic Engineering,
// * University of Cagliari,
// * P.zza D’Armi, 09123, Cagliari, Italy
// *
// * email: emanuele.manca@diee.unica.it
// *
// *
// * this software uses the cutil library of NVIDIA CUDA SDK
// *
// * This software contains source code provided by NVIDIA Corporation
// * license: http://developer.download.nvidia.com/licenses/general_license.txt
// *
// * This function are derived by NVIDIA Corporation:
// *
// * 	 1. scanInclusiveLarge()
// * 	 2. scanInclusiveShared()
// * 	 3. scanInclusiveShared2()
// * 	 4. scanInclusiveShort
// *
// *
// *
// */
//
//
//__global__ void scanInclusiveShared2(
//        uint *d_Buf,
//        uint *d_Dst,
//        uint N,
//        uint arrayLength
//){
//    __shared__ uint s_Data[2 * THREADBLOCK_SIZE];
//
//    //Skip loads and stores for inactive threads of last threadblock (pos >= N)
//    uint pos = blockIdx.x * blockDim.x + threadIdx.x;
//
//    //Load top elements
//    //Convert results of bottom-level scan back to inclusive
//    uint idata = 0;
//    if(pos < N)
//        idata =  d_Dst[(4 * THREADBLOCK_SIZE) - 1 + (4 * THREADBLOCK_SIZE) * pos];
//
//    //Compute
//    uint odata = scan1Exclusive(idata, s_Data, arrayLength);
//
//    //Avoid out-of-bound access
//    if(pos < N)
//        d_Buf[pos] = odata;
//}
//
//__global__ void scanInclusiveShared(
//        uint4 *d_Dst,
//        uint4 *d_Src,
//        uint size
//){
//    __shared__ uint s_Data[2 * THREADBLOCK_SIZE];
//
//    uint pos = blockIdx.x * blockDim.x + threadIdx.x;
//
//    //  if(pos<warpSize*(size/warpSize+1))
//    {
//        //Load data
//        uint4 idata4 = d_Src[pos];
//
//        //Calculate exclusive scan
//        uint4 odata4 = scan4Inclusive(idata4, s_Data, size);
//
//        //Write back
//        d_Dst[pos] = odata4;
//    }
//}
//
//size_t scanInclusiveShort(
//        uint *d_Dst,
//        uint *d_Src,
//        uint batchSize,
//        uint arrayLength
//){
//    //Check power-of-two factorization
//    uint log2L;
//    uint factorizationRemainder = factorRadix2(log2L, arrayLength);
//    assert( factorizationRemainder == 1 );
//
//    //Check supported size range
//    //  assert( (arrayLength >= MIN_SHORT_ARRAY_SIZE) && (arrayLength <= MAX_SHORT_ARRAY_SIZE) );
//
//    //Check total batch size limit
//    // assert( (batchSize * arrayLength) <= MAX_BATCH_ELEMENTS );
//
//    //Check all threadblocks to be fully packed with data
//    //assert( (batchSize * arrayLength) % (4 * THREADBLOCK_SIZE) == 0 );
//    int blocks=(batchSize * arrayLength + 4 * THREADBLOCK_SIZE-1 )/ (4 * THREADBLOCK_SIZE);
//    scanInclusiveShared<<<blocks, THREADBLOCK_SIZE>>>(
//            (uint4 *)d_Dst,
//            (uint4 *)d_Src,
//            arrayLength
//    );
////    getLastCudaError("scanExclusiveShared() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    return THREADBLOCK_SIZE;
//}
//
//size_t scanInclusiveLarge(
//        uint *d_Dst,
//        uint *d_Src,
//        uint batchSize,
//        uint arrayLength
//){
//    //Check power-of-two factorization
//    uint log2L;
//    uint factorizationRemainder = factorRadix2(log2L, arrayLength);
//    assert( factorizationRemainder == 1 );
//
//    //Check supported size range
//    //assert( (arrayLength >= MIN_LARGE_ARRAY_SIZE) && (arrayLength <= MAX_LARGE_ARRAY_SIZE) );
//
//    //Check total batch size limit
//    //assert( (batchSize * arrayLength) <= MAX_BATCH_ELEMENTS );
//
//    scanInclusiveShared<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE>>>(
//            (uint4 *)d_Dst,
//            (uint4 *)d_Src,
//            4 * THREADBLOCK_SIZE
//    );
////    getLastCudaError("scanExclusiveShared() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    //Not all threadblocks need to be packed with input data:
//    //inactive threads of highest threadblock just don't do global reads and writes
//    const uint blockCount2 = iDivUp( (batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE );
//    scanInclusiveShared2<<< blockCount2, THREADBLOCK_SIZE>>>(
//            (uint *)d_Buf,
//            (uint *)d_Dst,
//            (batchSize * arrayLength) / (4 * THREADBLOCK_SIZE),
//            arrayLength / (4 * THREADBLOCK_SIZE)
//    );
////    getLastCudaError("scanExclusiveShared2() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    uniformUpdate<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE>>>(
//            (uint4 *)d_Dst,
//            (uint  *)d_Buf
//    );
////    getLastCudaError("uniformUpdate() execution FAILED\n");
//    HANDLE_ERROR(cudaPeekAtLastError());
//
//    return THREADBLOCK_SIZE;
//}
//
//
//template <typename Type>
//__global__ void quick(Type* indata,Type* buffer,  Partition<Type>* partition, Block<Type>* bucket )
//{
//
//    __shared__ Type sh_out[1024];
//
//    __shared__ uint start1,end1;
//    __shared__ uint left,right;
//
//    int tix = threadIdx.x;
//
//    uint start  = partition[blockIdx.x].from;
//    uint end    = partition[blockIdx.x].end;
//    Type pivot  = partition[blockIdx.x].pivot;
//    uint nseq   = partition[blockIdx.x].ibucket;
//
//    uint lo=0;
//    uint hi=0;
//
//    Type lmin = 0xffffffff;
//    Type rmax = 0;
//
//    Type d;
//
//
//    // start read on 1° tile and store the coordinates of the items that must
//    // be moved on the left or on the right of the pivot
//
//    if(tix+start<end)
//    {
//        d = indata[tix+start];
//
//        //count items smaller or bigger than the pivot
//        // if d<pivot then ll++ else ll
//        lo=(d<pivot)*(lo+1)+(d>=pivot)*lo;
//        // if d>pivot then lr++ else lr
//        hi=(d<=pivot)*(hi)+(d>pivot)*(hi+1);
//
//        lmin = d;
//        rmax= d;
//    }
//
//    //read and store the coordinates on next tiles for each block
//    for(uint i=tix+start+blockDim.x;i<end;i+=blockDim.x)
//    {
//        Type d= indata[i];
//
//        //count items smaller or bigger than the pivot
//        lo = ( d <  pivot ) *(lo+1) + ( d >= pivot )*lo;
//        hi = ( d <= pivot ) *(hi)   +  (d >  pivot )*(hi+1);
//
//        //compute max and min of tile items
//        lmin = min(lmin,d);
//        rmax = max(rmax,d);
//
//    }
//
//    //compute max and min of every partition
//
//    compareInclusive(rmax,lmin,(Type*) sh_out, blockDim.x);
//
//    __syncthreads();
//
//    if(tix==blockDim.x-1)
//    {
//        //compute absolute max and min for the bucket
//        atomicMax(&bucket[nseq].maxPiv,rmax);
//        atomicMin(&bucket[nseq].minPiv,lmin);
//    }
//
//    __syncthreads();
//
//
//    /*
//     * calculate the coordinates of its assigned item to each thread,
//     * which are necessary to known in which subsequences the item must be copied
//     *
//     */
//    scan1Inclusive2(lo,hi,(uint*) sh_out, blockDim.x);
//    lo = lo-1;
//    hi = SHARED_LIMIT-hi;
//
//
//    if(tix==blockDim.x-1)
//    {
//        left  = lo+1;
//        right = SHARED_LIMIT-hi;
//
//        start1 = atomicAdd(&bucket[nseq].nextbegin,left);
//        end1   = atomicSub(&bucket[nseq].nextend, right);
//    }
//
//    __syncthreads();
//
//
//    //thread blocks write on the shared memory the items smaller and bigger than the first tile's pivot
//    if(tix+start<end)
//    {
//        //items smaller than pivot
//        if(d<pivot)
//        {sh_out[lo]=d; lo--;}
//
//        //items bigger than pivot
//        if(d>pivot)
//        {sh_out[hi]=d; hi++;}
//
//    }
//
//    //thread blocks write on the shared memory the items smaller and bigger than next tiles' pivot
//    for(uint i=start+tix+blockDim.x;i<end;i+=blockDim.x)
//    {
//
//        Type d=indata[i];
//        //items smaller than the pivot
//        if(d<pivot)
//        {sh_out[lo]=d; lo--;}
//
//        //items bigger than the pivot
//        if(d>pivot)
//        {sh_out[hi]=d; hi++;}
//
//    }
//
//    __syncthreads();
//
//    //items smaller and bigger than the pivot already sorted in the shared memory are coalesced written on the global memory
//    //partial results of each thread block stored on the shared memory are merged together in two subsequences within the global memory
//    //coalesced writing of next tiles on the global memory
//    for(uint i=tix ;i<SHARED_LIMIT;i+=blockDim.x)
//    {
//        if(i<left)
//            buffer[start1+i]=sh_out[i];
//
//        if(i>=SHARED_LIMIT-right)
//            buffer[end1+i-SHARED_LIMIT]=sh_out[i];
//    }
//
//}
//
//
//
////this function assigns the attributes to each partition of each bucket
////a thread block is assigned to a specific partition
//template <typename Type>
//__global__ void partitionAssign(struct Block<Type>* bucket,uint* npartitions,struct Partition<Type>* partition)
//{
//    int tx=threadIdx.x;
//    int bx=blockIdx.x;
//
//    uint beg   = bucket[bx].nextbegin;
//    uint end   = bucket[bx].nextend;
//    Type pivot = bucket[bx].pivot;
//
//    uint from;
//    uint to;
//
//    if(bx>0)
//    {
//        from=npartitions[bx-1];
//        to=npartitions[bx];
//    }
//    else
//    {
//        from=0;
//        to=npartitions[bx];
//    }
//
//
//    uint i=tx+from;
//
//    if(i<to )
//    {
//        uint begin=beg+SHARED_LIMIT*tx;
//        partition[i].from=begin;
//        partition[i].end=begin+SHARED_LIMIT;
//        partition[i].pivot=pivot;
//        partition[i].ibucket=bx;
//
//    }
//
//
//    for(uint i=tx+from+blockDim.x;i<to ;i+=blockDim.x)
//    {
//        uint begin=beg+SHARED_LIMIT*(i-from);
//        partition[i].from=begin;
//        partition[i].end=begin+SHARED_LIMIT;
//        partition[i].pivot=pivot;
//        partition[i].ibucket=bx;
//    }
//    __syncthreads();
//    if(tx==0 && to-from>0) partition[to-1].end=end;
//
//
//}
//
////this function enters the pivot value in the central bucket's items
//template <typename Type>
//__global__ void insertPivot(Type* data,struct Block<Type>* bucket,int nbucket)
//{
//
//    Type pivot      = bucket[blockIdx.x].pivot;
//    uint start      = bucket[blockIdx.x].nextbegin;
//    uint end        = bucket[blockIdx.x].nextend;
//    bool is_altered = bucket[blockIdx.x].done;
//
//    if(is_altered && blockIdx.x<nbucket)
//        for(uint j=start+threadIdx.x; j<end; j+=blockDim.x)
//            data[j]=pivot;
//
//
//}
//
//
////this function assigns the new attributes of each bucket
//template <typename Type>
//__global__ void bucketAssign(Block<Type>* bucket,uint*npartitions,int nbucket,int select)
//{
//
//    uint i=blockIdx.x*blockDim.x+threadIdx.x;
//
//    if(i<nbucket){
//        bool is_altered=bucket[i].done;
//        if(is_altered )
//        {
//            //read on i node
//            uint orgbeg = bucket[i].begin;
//            uint from    = bucket[i].nextbegin;
//            uint orgend = bucket[i].end;
//            uint end    = bucket[i].nextend;
//            Type pivot  = bucket[i].pivot;
//            Type minPiv = bucket[i].minPiv;
//            Type maxPiv = bucket[i].maxPiv;
//
//            //compare each bucket's max and min to the pivot
//            Type lmaxpiv = min(pivot,maxPiv);
//            Type rminpiv = max(pivot,minPiv);
//
//            //write on i+nbucket node
//            bucket[i+nbucket].begin = orgbeg;
//            bucket[i+nbucket].nextbegin   = orgbeg;
//            bucket[i+nbucket].nextend    = from;
//            bucket[i+nbucket].end = from;
//            bucket[i+nbucket].pivot  = (minPiv+lmaxpiv)/2;
//
//            //if(select)
//            //	bucket[i+nbucket].done   = (from-orgbeg)>1024;// && (minPiv!=maxPiv);
//            //else
//            bucket[i+nbucket].done   = (from-orgbeg)>1024 && (minPiv!=maxPiv);
//            bucket[i+nbucket].select=select;
//            bucket[i+nbucket].minPiv = 0xffffffffffffffff;
//            bucket[i+nbucket].maxPiv = 0;
//            //bucket[i+nbucket].finish=false;
//
//            //calculate the number of partitions (npartitions) necessary to the i+nbucket bucket
//            if(!bucket[i+nbucket].done)
//                npartitions[i+nbucket] = 0;
//            else npartitions[i+nbucket] = (from-orgbeg+SHARED_LIMIT-1)/SHARED_LIMIT;
//
//            //write on i node
//            bucket[i].begin = end;
//            bucket[i].nextbegin   = end;
//            bucket[i].nextend    = orgend;
//            bucket[i].pivot  = (rminpiv+maxPiv)/2+1;
//
//            //if(select)
//            //bucket[i].done   = (orgend-end)>1024;// && (minPiv!=maxPiv);
//            //	else
//            bucket[i].done   = (orgend-end)>1024 && (minPiv!=maxPiv);
//            bucket[i].select=select;
//            bucket[i].minPiv = 0xffffffffffffffff;
//            bucket[i].maxPiv = 0;
//            //bucket[i].finish=false;
//
//            //calculate the number of partitions (npartitions) necessary to the i-bucket bucket
//            if(!bucket[i].done)
//                npartitions[i]=0;
//            else
//                npartitions[i]=(orgend-end+SHARED_LIMIT-1)/SHARED_LIMIT;
//
//        }
//    }
//
//
//}
//
//
//
//template <typename Type>
//__global__ void init(Type* data,Block<Type>* bucket,uint* npartitions,int size, int nblocks)
//{
//    uint i=blockIdx.x*blockDim.x+threadIdx.x;
//
//    if(i<nblocks)
//    {
//        bucket[i].nextbegin   = 0;
//        bucket[i].begin = 0;
//
//        bucket[i].nextend    = 0 + size*(i==0);
//        bucket[i].end = 0 + size*(i==0);
//        npartitions[i]   = 0;
//        bucket[i].done   = false + i==0;
//        bucket[i].select   = false;
//        bucket[i].maxPiv = 0x0;
//        bucket[i].minPiv = 0xffffffffffffffff;
//        //bucket[i].pivot  = 0+ (i==0)*((min(min(data[0],data[size/2]),data[size-1]) + max(max(data[0],data[size/2]),data[size-1]))/2);
//        bucket[i].pivot = data[size/2];
//    }
//
//}
//
//
//
//template <typename Type>
//void sort(Type *inputData, Type *outputData, uint size, uint threadCount, int device)
//{
//
//    cudaSetDevice(device);
//
//    cudaGetLastError();
//    //cudaDeviceReset();
//
//    cudaDeviceProp deviceProp;
//    cudaGetDeviceProperties(&deviceProp, device);
//
//    Type* ddata;
//    Type* dbuffer;
//
//    Block<Type>* dbucket;
//    struct Partition<Type>* partition;
//    uint* npartitions1,*npartitions2;
//
//    uint*cudaBlocks=(uint*)malloc(4);
//
//    uint blocks = (size + SHARED_LIMIT-1)/SHARED_LIMIT;
//    uint nblock=10*blocks;
//    int partition_max= 262144;
//
//    unsigned long long int total = partition_max*sizeof(Block<Type>) + nblock*sizeof(Partition<Type>) + 2*partition_max*sizeof(uint) +3*(size)*sizeof(Type);
//
//    printf("\nINFO: Device Memory consumed is %.3f GB out of %.3f GB of available memory\n", ((double)total/GIGA), (double)deviceProp.totalGlobalMem/GIGA);
//
//    //Allocating and initializing CUDA arrays
////	sdkCreateTimer(&htimer);
//    HANDLE_ERROR( cudaMalloc  ((void**)&dbucket   , partition_max*sizeof(Block<Type>)) );
//    HANDLE_ERROR( cudaMalloc  ((void**)&partition , nblock*sizeof(Partition<Type>)) ); //nblock
//
//    HANDLE_ERROR(cudaMalloc((void**)&npartitions1,partition_max*sizeof(uint)) );
//    HANDLE_ERROR(cudaMalloc((void**)&npartitions2,partition_max*sizeof(uint)) );
//
//    HANDLE_ERROR(cudaMalloc((void**)&dbuffer,(size)*sizeof(Type)));
//    HANDLE_ERROR(cudaMalloc((void**)&ddata  ,(size)*sizeof(Type)));
//
//    HANDLE_ERROR(cudaMemcpy(ddata, inputData, size*sizeof(Type), cudaMemcpyHostToDevice) );
//
//    initScan();
//
//    //setting GPU Cache
//    cudaFuncSetCacheConfig(init<Type>,           	 cudaFuncCachePreferL1);
//    cudaFuncSetCacheConfig(insertPivot<Type>,    	 cudaFuncCachePreferL1);
//    cudaFuncSetCacheConfig(bucketAssign<Type>,   	 cudaFuncCachePreferL1);
//    cudaFuncSetCacheConfig(partitionAssign<Type>, 	 cudaFuncCachePreferL1);
//    cudaFuncSetCacheConfig(quick<Type>,          	 cudaFuncCachePreferShared);
//    cudaFuncSetCacheConfig(globalBitonicSort<Type>,  cudaFuncCachePreferShared);
//
//
//    HANDLE_ERROR(cudaDeviceSynchronize());
////    sdkResetTimer(&htimer);
////    sdkStartTimer(&htimer);
//
//    //initializing bucket array: initial attributes for each bucket
//    init<Type><<<(nblock+255)/256,256>>>(ddata,dbucket,npartitions1,size,partition_max);
//
//
//    uint nbucket     = 1;
//    uint numIterations  = 0;
//    bool inputSelect = true;
//
//    *cudaBlocks=blocks;
//    HANDLE_ERROR(cudaDeviceSynchronize());
////	getLastCudaError("init() execution FAILED\n");
//    HANDLE_ERROR( cudaMemcpy(&npartitions2[0], cudaBlocks,  sizeof(uint), cudaMemcpyHostToDevice) );
//
//
//    // beginning of the first phase
//    // this phase goes on until the size of the buckets is comparable to the SHARED_LIMIT size
//    while(1)
//    {
//
//        /*
//         *       	---------------------    Pre-processing: Partitioning    ---------------------
//         *
//         * buckets are further divided in partitions based on their size
//         * the number of partitions needed for each subsequence is determined by the number of elements which can be
//         * processed by each thread block.
//         *
//         * the number of partitions (npartitions) for each block will depend on the shared memory size (SHARED_LIMIT)
//         *
//         */
//
//        if(numIterations>0)
//        {	//1024 is the shared memory limit of scanInclusiveShort()
//            if(nbucket<=1024)
//                scanInclusiveShort(npartitions2, npartitions1, 1, nbucket);
//            else
//                scanInclusiveLarge(npartitions2, npartitions1, 1, nbucket);
//
//            HANDLE_ERROR( cudaMemcpy(cudaBlocks, &npartitions2[nbucket-1],  sizeof(uint), cudaMemcpyDeviceToHost) );
//        }
//
//        if(*cudaBlocks==0)
//            break;
//
//
//        /*
//         *  ---------------------     step 1    ---------------------
//         *
//         * 	A thread block is assigned to each different partition
//         * 	each partition is assigned coordinates, pivot and ....
//         */
//
//
//        partitionAssign<Type><<<nbucket,1024>>>(dbucket,npartitions2,partition);
//        cudaDeviceSynchronize();
////		getLastCudaError("partitionAssign() execution FAILED\n");
//
//        /*
//              ---------------------    step 2a    ---------------------
//
//              in this function each thread block creates two subsequences
//              to divide the items in the partition whose value is lower than
//             the pivot value, from the items whose value is higher than the pivot value
//         */
//
//        if(inputSelect)
//            quick<Type><<<*cudaBlocks,threadCount>>>(ddata,dbuffer,partition,dbucket);
//        else
//            quick<Type><<<*cudaBlocks,threadCount>>>(dbuffer,ddata,partition,dbucket);
//        cudaDeviceSynchronize();
////		getLastCudaError("quick() execution FAILED\n");
//
//        //step 2b: this function enters the pivot value in the central bucket's items
//        insertPivot<Type><<<nbucket,512>>>(ddata,dbucket,nbucket);
//
//
//        //step 3: parameters are assigned, linked to the two new buckets created in step 2
//        bucketAssign<Type><<<(nbucket+255)/256,256>>>(dbucket,npartitions1,nbucket,inputSelect);
//        cudaDeviceSynchronize();
////		getLastCudaError("insertPivot() or bucketAssign() execution FAILED\n");
//
//        nbucket*=2;
//
//        inputSelect = !inputSelect;
//        numIterations++;
//        if(nbucket>deviceProp.maxGridSize[0])
//            break;
//        //if(numIterations==18) break;
//    }
//
//    /*
//     * start second phase:
//     * now the size of the buckets is such that they can be entirely processed by a thread block
//     *
//     */
//
//    if(nbucket>deviceProp.maxGridSize[0])
//        fprintf(stderr, "ERROR: CUDA-Quicksort can't terminate sorting as the block threads needed to finish it are more than the Maximum x-dimension of FERMI GPU thread blocks. Please use Kepler GPUs as the Maximum x-dimension of their thread blocks is much higher\n");
//    else
//        globalBitonicSort<Type><<<nbucket,512,0>>>(ddata,dbuffer,dbucket,inputSelect);
//
//    HANDLE_ERROR(cudaDeviceSynchronize());
////	getLastCudaError("globalBitonicSort() execution FAILED\n");
//
////	sdkStopTimer(&htimer);
////    *wallClock=sdkGetTimerValue(&htimer);
//
//
//    // Copy the final result to the CPU in the outputData array
//    HANDLE_ERROR(cudaMemcpy(outputData, ddata, size*sizeof(Type), cudaMemcpyDeviceToHost) );
//
//    // release resources
//    HANDLE_ERROR( cudaFree(ddata) );
//    HANDLE_ERROR( cudaFree(dbuffer) );
//    HANDLE_ERROR( cudaFree(dbucket) );
//    HANDLE_ERROR( cudaFree(npartitions2));
//    HANDLE_ERROR( cudaFree(npartitions1));
//    free(cudaBlocks);
//
//    closeScan();
//    return ;
//}
//
//
//
//void CUDA_Quicksort(Metrics* inputData, Metrics* outputData, uint dataSize, uint threadCount /*, int Device, double* wallClock*/)
//{
//
//    cudaDeviceProp deviceProp;
//    cudaGetDeviceProperties(&deviceProp, 0);
//
//    if(deviceProp.major<2)
//    {
//        fprintf(stderr, "Error: the GPU device %d has a Compute Capability of %d.%d, while a Compute Capability of 2.x is required to run the code\n",
//                0, deviceProp.major, deviceProp.minor);
//
//        int deviceCount;
//        cudaGetDeviceCount(&deviceCount);
//
//        fprintf(stderr, "       the Host system has the following GPU devices:\n");
//
//        for (int device = 0; device < deviceCount; device++) {
//
//            fprintf(stderr, "\t  the GPU device %d is a %s, with Compute Capability %d.%d\n",
//                    device, deviceProp.name, deviceProp.major, deviceProp.minor);
//        }
//
//        return;
//    }
//
//    sort<Metrics>(inputData,outputData, dataSize,threadCount,0);
//}
////
////void CUDA_Quicksort_64(double* inputData,double* outputData, uint dataSize, uint threadCount, int Device, double* wallClock)
////{
////
////    cudaDeviceProp deviceProp;
////    cudaGetDeviceProperties(&deviceProp, Device);
////
////    if(deviceProp.major<2)
////    {
////        fprintf(stderr, "Error: the GPU device %d has a Compute Capability of %d.%d, while a Compute Capability of 2.x is required to run the code\n",
////                Device, deviceProp.major, deviceProp.minor);
////
////        int deviceCount;
////        cudaGetDeviceCount(&deviceCount);
////
////        fprintf(stderr, "       the Host system has the following GPU devices:\n");
////
////        for (int device = 0; device < deviceCount; device++) {
////
////            fprintf(stderr, "\t  the GPU device %d is a %s, with Compute Capability %d.%d\n",
////                    device, deviceProp.name, deviceProp.major, deviceProp.minor);
////        }
////
////        return;
////    }
////
////    sort<double>(inputData,outputData, dataSize,threadCount,Device);
////
////}